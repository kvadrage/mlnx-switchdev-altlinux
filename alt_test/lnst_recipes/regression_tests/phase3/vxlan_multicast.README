Topology:

                         switch
                        +------+
                        |      |
                        |      |
          +-------------+      +-------------+
          |             |      |             |
          |             |      |             |
          |             +------+             |
          |                                  |
          |                                  |
        +-+--+                             +-+--+
+-------|eth1|------+              +-------|eth1|------+
|host1  +---++      |              |host2  +-+--+      |
|           |       |              |         |         |
|           |       |              |      +-----+      |
|+-----+  +-+-+     |              |      |vxlan|      |
||vxlan|--+br0|     |              |      +-----+      |
|+-----+  +-+-+     |              |                   |
|           |       |              |                   |
|           |       |              |                   |
|           |       |              |                   |
|         +-+-+     |              |                   |
+---------|tap|-----+              +-------------------+
          +-+-+
            |
          +-+--+
+---------|eth1|----+
|guest1   +-+--+    |
|           |       |
|        +--+--+    |
|        |vxlan|    |
|        +-----+    |
|                   |
+---------+---------+

Number of hosts: 3
Host #1 description:
    One ethernet device
    Tap device connecting to a guest machine
    Bridge br0 enslaving eth1 and tap devices, configured with ip address
        {$net}.1/24
    VXLAN interface on top of the bridge interface using group_ip 239.1.1.1
        configured with ip addresses:
        {$vxlan_net}.1/24
        {$vxlan_net6}::1/64
Guest #1 description:
    One ethernet device configured with ip address {$net}.2/24
    VXLAN interface on top of the ethernet interface using group_ip 239.1.1.1
        configured with ip addresses:
        {$vxlan_net}.2/24
        {$vxlan_net6}::2/64
Host #2 description:
    One ethernet device configured with ip address {$net}.3/24
    VXLAN interface on top of the ethernet interface using group_ip 239.1.1.1
        configured with ip addresses:
        {$vxlan_net}.3/24
        {$vxlan_net6}::3/64
Test name:
    vxlan_test.py
Test description:
    Ping:
      + count: 100
      + interval: 0.1s
      + host1.vxlan -> host2.vxlan expecting PASS
      + host1.vxlan -> guest1.vxlan expecting PASS
      + host2.vxlan -> host1.vxlan expecting PASS
      + host2.vxlan -> guest1.vxlan expecting PASS
      + guest1.vxlan -> host1.vxlan expecting PASS
      + guest1.vxlan -> host2.vxlan expecting PASS
      All pings are executed in parallel
    Ping6:
      + count: 100
      + interval: 0.1s
      + host1.vxlan -> host2.vxlan expecting PASS
      + host1.vxlan -> guest1.vxlan expecting PASS
      + host2.vxlan -> host1.vxlan expecting PASS
      + host2.vxlan -> guest1.vxlan expecting PASS
      + guest1.vxlan -> host1.vxlan expecting PASS
      + guest1.vxlan -> host2.vxlan expecting PASS
      All pings are executed in parallel
    Netperf:
      + duration: 60s, repeated 5 times to calculate confidence
      + host1.vxlan -> host2.vxlan TCP_STREAM ipv4
      + host1.vxlan -> host2.vxlan UDP_STREAM ipv4
      + host1.vxlan -> host2.vxlan TCP_STREAM ipv6
      + host1.vxlan -> host2.vxlan UDP_STREAM ipv6

PerfRepo integration:
    First, preparation in PerfRepo is required - you need to create Test objects
    through the web interface that properly describe the individual Netperf
    tests that this recipe runs. Don't forget to also add appropriate metrics.
    For these Netperf tests it's always:
    * throughput
    * throughput_min
    * throughput_max
    * throughput_deviation

    After that, to enable support for PerfRepo you need to create the file
    vxlan_multicast.mapping and define the following id mappings:
    tcp_ipv4_id -> to store ipv4 TCP_STREAM Netperf test results, maps to TestUid of a PerfRepo Test object
    tcp_ipv6_id -> to store ipv6 TCP_STREAM Netperf test results, maps to TestUid of a PerfRepo Test object
    udp_ipv4_id -> to store ipv4 UDP_STREAM Netperf test results, maps to TestUid of a PerfRepo Test object
    udp_ipv6_id -> to store ipv4 UDP_STREAM Netperf test results, maps to TestUid of a PerfRepo Test object

    To enable result comparison agains baselines you need to create a Report in
    PerfRepo that will store the baseline. Set up the Report to only contain results
    with the same hash tag and then add a new mapping to the mapping file, with
    this format:
    <some_hash> = <report_id>

    The hash value is automatically generated during test execution and added
    to each result stored in PerfRepo. To get the Report id you need to open
    that report in our browser and find if in the URL.

    When running this recipe you should also define the 'product_name' alias
    (e.g. RHEL7) in order to tag the result object in PerfRepo.
